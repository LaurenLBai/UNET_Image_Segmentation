{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29a6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation-models in c:\\users\\garre\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from segmentation-models) (1.0.8)\n",
      "Requirement already satisfied: efficientnet==1.0.0 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from segmentation-models) (1.0.0)\n",
      "Requirement already satisfied: image-classifiers==1.0.0 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from segmentation-models) (1.0.0)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\garre\\anaconda3\\lib\\site-packages (from efficientnet==1.0.0->segmentation-models) (0.19.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\garre\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.21.5)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.9.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (9.2.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (21.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2021.7.2)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.7.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.7.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\garre\\anaconda3\\lib\\site-packages (from packaging>=20.0->scikit-image->efficientnet==1.0.0->segmentation-models) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# Installing tensorflow \n",
    "! pip install segmentation-models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396b05e",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc5fc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "import random\n",
    "\n",
    "\n",
    "#Now, crop each large image into patches of 256x256. Save them into a directory \n",
    "#so we can use data augmentation and read directly from the drive. \n",
    "root_directory = '../../Datasets/Shoreline_Dataset/'\n",
    "\n",
    "patch_size = 256\n",
    "\n",
    "#Read images from repsective 'images' subdirectory\n",
    "#As all images are of different size we have 2 options, either resize or crop\n",
    "#But, some images are too large and some small. Resizing will change the size of real objects.\n",
    "#Therefore, we will crop them to a nearest size divisible by 256 and then \n",
    "#divide all images into patches of 256x256x3. \n",
    "# img_dir=root_directory+\"images/\"\n",
    "# for path, subdirs, files in os.walk(img_dir):\n",
    "#     #print(path)  \n",
    "#     dirname = path.split(os.path.sep)[-1]\n",
    "#     #print(dirname)\n",
    "#     images = os.listdir(path)  #List of all image names in this subdirectory\n",
    "#     #print(images)\n",
    "#     id = 0\n",
    "#     for i, image_name in enumerate(images):  \n",
    "#         if image_name.endswith(\".JPG\"):\n",
    "#             #print(image_name)\n",
    "#             image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
    "#             SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "#             SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "#             image = Image.fromarray(image)\n",
    "#             image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "#             #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "#             image = np.array(image)             \n",
    "   \n",
    "#             #Extract patches from each image\n",
    "#             print(\"Now patchifying image:\", path+\"/\"+image_name)\n",
    "#             patches_img = patchify(image, (256, 256, 3), step=256)  #Step=256 for 256 patches means no overlap\n",
    "            \n",
    "            \n",
    "#             for i in range(patches_img.shape[0]):\n",
    "#                 for j in range(patches_img.shape[1]):\n",
    "#                     id += 1\n",
    "#                     single_patch_img = patches_img[i,j,:,:]\n",
    "#                     #single_patch_img = (single_patch_img.astype('float32')) / 255. #We will preprocess using one of the backbones\n",
    "#                     single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "                    \n",
    "#                     cv2.imwrite(root_directory+\"256_patches/images/\"+\"patch_\"+str(id)+\".png\", single_patch_img)\n",
    "#                     #image_dataset.append(single_patch_img)\n",
    "            \n",
    "   \n",
    " #Now do the same as above for masks\n",
    " #For this specific dataset we could have added masks to the above code as masks have extension png\n",
    "# mask_dir=root_directory+\"masks/\"\n",
    "# for path, subdirs, files in os.walk(mask_dir):\n",
    "#     #print(path)  \n",
    "#     dirname = path.split(os.path.sep)[-1]\n",
    "\n",
    "#     masks = os.listdir(path)  #List of all image names in this subdirectory\n",
    "#     id = 0\n",
    "#     for i, mask_name in enumerate(masks):  \n",
    "#         if mask_name.endswith(\".png\"):           \n",
    "#             mask = cv2.imread(path+\"/\"+mask_name, 0)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
    "#             SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "#             SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "#             mask = Image.fromarray(mask)\n",
    "#             mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "#             #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "#             mask = np.array(mask)             \n",
    "   \n",
    "#             #Extract patches from each image\n",
    "#             print(\"Now patchifying mask:\", path+\"/\"+mask_name)\n",
    "#             patches_mask = patchify(mask, (256, 256), step=256)  #Step=256 for 256 patches means no overlap\n",
    "            \n",
    "#             for i in range(patches_mask.shape[0]):\n",
    "#                 for j in range(patches_mask.shape[1]):\n",
    "#                     id += 1\n",
    "#                     single_patch_mask = patches_mask[i,j,:,:]\n",
    "#                     #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
    "#                     #single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "#                     cv2.imwrite(root_directory+\"256_patches/masks/\"+\"patch_\"+ str(id) +\".png\", single_patch_mask)\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c41d4",
   "metadata": {},
   "source": [
    "# Select images with useful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d890b375",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m train_img_dir \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Datasets/Shoreline_Dataset/256_patches/images/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m train_mask_dir \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Datasets/Shoreline_Dataset/256_patches/masks/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m img_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(train_img_dir)\n\u001b[0;32m      8\u001b[0m msk_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(train_mask_dir)\n\u001b[0;32m     10\u001b[0m num_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(train_img_dir))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#Now, let us copy images and masks with real information to a new folder.\n",
    "# real information = if mask has decent amount of labels other than 0. \n",
    "\n",
    "train_img_dir =  \"../Datasets/Shoreline_Dataset/256_patches/images/\"\n",
    "train_mask_dir =  \"../Datasets/Shoreline_Dataset/256_patches/masks/\"\n",
    "\n",
    "img_list = os.listdir(train_img_dir)\n",
    "msk_list = os.listdir(train_mask_dir)\n",
    "\n",
    "num_images = len(os.listdir(train_img_dir))\n",
    "\n",
    "\n",
    "img_num = random.randint(0, num_images-1)\n",
    "\n",
    "img_for_plot = cv2.imread(train_img_dir+img_list[img_num], 1)\n",
    "img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_for_plot =cv2.imread(train_mask_dir+msk_list[img_num], 0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_for_plot)\n",
    "plt.title('Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_for_plot, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.show()\n",
    "\n",
    "# useless=0  #Useless image counter\n",
    "# for img in range(len(img_list)):   #Using t1_list as all lists are of same size\n",
    "#     img_name=img_list[img]\n",
    "#     mask_name = msk_list[img]\n",
    "#     print(\"Now preparing image and masks number: \", img)\n",
    "      \n",
    "#     temp_image=cv2.imread(train_img_dir+img_list[img], 1)\n",
    "   \n",
    "#     temp_mask=cv2.imread(train_mask_dir+msk_list[img], 0)\n",
    "#     #temp_mask=temp_mask.astype(np.uint8)\n",
    "    \n",
    "#     val, counts = np.unique(temp_mask, return_counts=True)\n",
    "    \n",
    "#     if (1 - (counts[0]/counts.sum())) > 0.05:  #At least 5% useful area with labels that are not 0\n",
    "#         print(\"Save Me\")\n",
    "#         cv2.imwrite('../Datasets/Shoreline_Dataset/useful_images/images/'+img_name, temp_image)\n",
    "#         cv2.imwrite('../Datasets/Shoreline_Dataset/useful_images/masks/'+mask_name, temp_mask)\n",
    "        \n",
    "#     else:\n",
    "#         print(\"I am useless\")   \n",
    "#         useless +=1\n",
    "\n",
    "# print(\"Total useful images are: \", len(img_list)-useless)  #20,075\n",
    "# print(\"Total useless images are: \", useless) #21,571"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22bbdd",
   "metadata": {},
   "source": [
    "# Split the folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d062c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders  # or import split_folders\n",
    "\n",
    "input_folder = \"../Datasets/Shoreline_Dataset/useful_images/\"\n",
    "output_folder = '../Datasets/Shoreline_Ready_Data/'\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.5, .5), group_prefix=None) # default values\n",
    "########################################\n",
    "\n",
    "#Now manually move folders around to bring them to the following structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d786bb",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "import random\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution() #in case the model gets very slow, may be due to a bug in TF2.0. Uncomment this. \n",
    "#https://github.com/tensorflow/tensorflow/issues/33024\n",
    "\n",
    "#Also check this in case you notice training to be getting increasingly slow each epoch.\n",
    "# https://stackoverflow.com/questions/53683164/keras-occupies-an-indefinitely-increasing-amount-of-memory-for-each-epoch\n",
    "\n",
    "################################################################\n",
    "#Get an understanding by looking at a few random images and masks \n",
    "\n",
    "train_img_dir = \"../Datasets/Shoreline_Ready_Data/train_images/train/\"\n",
    "train_mask_dir = \"../Datasets/Shoreline_Ready_Data/train_masks/train/\"\n",
    "\n",
    "img_list = os.listdir(train_img_dir)\n",
    "msk_list = os.listdir(train_mask_dir)\n",
    "\n",
    "num_images = len(os.listdir(train_img_dir))\n",
    "\n",
    "\n",
    "img_num = random.randint(0, num_images-1)\n",
    "\n",
    "img_for_plot = cv2.imread(train_img_dir+img_list[img_num], 1)\n",
    "img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_for_plot =cv2.imread(train_mask_dir+msk_list[img_num], 0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_for_plot)\n",
    "plt.title('Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_for_plot, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.show()\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Define Generator for images and masks so we can read them directly from the drive. \n",
    "\n",
    "seed=24\n",
    "batch_size= 16\n",
    "n_classes=1\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Use this to preprocess input for transfer learning\n",
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "#Define a function to perform additional preprocessing after datagen.\n",
    "#For example, scale images, convert masks to categorical, etc. \n",
    "def preprocess_data(img, mask, num_class):\n",
    "    #Scale images\n",
    "    img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
    "    img = preprocess_input(img)  #Preprocess based on the pretrained backbone...\n",
    "    #Convert mask to one-hot\n",
    "    # mask = to_categorical(mask, num_class)\n",
    "      \n",
    "    return (img,mask)\n",
    "\n",
    "#Define the generator.\n",
    "#We are not doing any rotation or zoom to make sure mask values are not interpolated.\n",
    "#It is important to keep pixel values in mask as 0, 1, 2, 3, .....\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "def trainGenerator(train_img_path, train_mask_path, num_class):\n",
    "    \n",
    "    img_data_gen_args = dict(horizontal_flip=True,\n",
    "                      vertical_flip=True,\n",
    "                      fill_mode='reflect')\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_img_path,\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_mask_path,\n",
    "        class_mode = None,\n",
    "        color_mode = 'grayscale',\n",
    "        batch_size = batch_size,\n",
    "        seed = seed)\n",
    "    \n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img, mask) in train_generator:\n",
    "        img, mask = preprocess_data(img, mask, num_class)\n",
    "        yield (img, mask)\n",
    "\n",
    "\n",
    "train_img_path = \"../Datasets/Shoreline_Ready_Data/train_images/\"\n",
    "train_mask_path = \"../Datasets/Shoreline_Ready_Data/train_masks/\"\n",
    "train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=n_classes)\n",
    "\n",
    "val_img_path = \"../Datasets/Shoreline_Ready_Data/val_images/\"\n",
    "val_mask_path = \"../Datasets/Shoreline_Ready_Data/val_masks/\"\n",
    "val_img_gen = trainGenerator(val_img_path, val_mask_path, num_class=n_classes)\n",
    "\n",
    "#Make sure the generator is working and that images and masks are indeed lined up. \n",
    "#Verify generator.... In python 3 next() is renamed as __next__()\n",
    "x, y = train_img_gen.__next__()\n",
    "\n",
    "# for i in range(0,3):\n",
    "#     image = x[i]\n",
    "#     mask = np.argmax(y[i], axis=2)\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.imshow(image)\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(mask, cmap='gray')\n",
    "#     plt.show()\n",
    "\n",
    "# x_val, y_val = val_img_gen.__next__()\n",
    "\n",
    "# for i in range(0,3):\n",
    "#     image = x_val[i]\n",
    "#     mask = np.argmax(y_val[i], axis=2)\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.imshow(image)\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(mask, cmap='gray')\n",
    "#     plt.show()\n",
    "\n",
    "###########################################################################\n",
    "#Define the model metrcis and load model. \n",
    "\n",
    "num_train_imgs = len(os.listdir('../Datasets/Shoreline_Ready_Data/train_images/train/'))\n",
    "num_val_images = len(os.listdir('../Datasets/Shoreline_Ready_Data/val_images/val/'))\n",
    "steps_per_epoch = num_train_imgs//batch_size\n",
    "val_steps_per_epoch = num_val_images//batch_size\n",
    "\n",
    "\n",
    "IMG_HEIGHT = x.shape[1]\n",
    "IMG_WIDTH  = x.shape[2]\n",
    "IMG_CHANNELS = x.shape[3]\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#Use transfer learning using pretrained encoder in the U-Net\n",
    "#(make sure you uncomment the preprocess_input part in the\n",
    "# preprocess_data function above)\n",
    "################################################################\n",
    "#Define the model\n",
    "# define model\n",
    "model = sm.Unet(BACKBONE, encoder_weights='imagenet', \n",
    "                input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "                classes=n_classes, activation='sigmoid')\n",
    "model.compile('Adam', loss= sm.losses.DiceLoss(), metrics=[sm.metrics.iou_score])\n",
    "\n",
    "#Other losses to try: categorical_focal_dice_loss, cce_jaccard_loss, cce_dice_loss, categorical_focal_loss\n",
    "\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "# print(model.summary())\n",
    "# print(model.input_shape)\n",
    "#Fit the model\n",
    "#history = model.fit(my_generator, validation_data=validation_datagen, steps_per_epoch=len(X_train) // 16, validation_steps=len(X_train) // 16, epochs=100)\n",
    "#Train the model. \n",
    "history=model.fit(train_img_gen,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=val_img_gen,\n",
    "          validation_steps=val_steps_per_epoch)\n",
    "\n",
    "model.save('landcover_25_epochs_RESNET_backbone_batch16.hdf5')\n",
    "\n",
    "##################################################################\n",
    "#plot the training and validation IoU and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['iou_score']\n",
    "val_acc = history.history['val_iou_score']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training IoU')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation IoU')\n",
    "plt.title('Training and validation IoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce0b88",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"landcover_25_epochs_RESNET_backbone_batch16.hdf5\", compile=False)\n",
    "\n",
    "#batch_size=32 #Check IoU for a batch of images\n",
    "\n",
    "#Test generator using validation data.\n",
    "\n",
    "test_image_batch, test_mask_batch = val_img_gen.__next__()\n",
    "\n",
    "#Convert categorical to integer for visualization and IoU calculation\n",
    "test_mask_batch_argmax = np.argmax(test_mask_batch, axis=3) \n",
    "test_pred_batch = model.predict(test_image_batch)\n",
    "test_pred_batch_argmax = np.argmax(test_pred_batch, axis=3)\n",
    "\n",
    "n_classes = 1\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(test_pred_batch_argmax, test_mask_batch_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "#######################################################\n",
    "#View a few images, masks and corresponding predictions. \n",
    "img_num = random.randint(0, test_image_batch.shape[0]-1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_image_batch[img_num])\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(test_mask_batch_argmax[img_num])\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(test_pred_batch_argmax[img_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4dc5b",
   "metadata": {},
   "source": [
    "# Smooth predictions utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    PLOT_PROGRESS = True\n",
    "    # See end of file for the rest of the __main__.\n",
    "else:\n",
    "    PLOT_PROGRESS = False\n",
    "\n",
    "\n",
    "def _spline_window(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Squared spline (power=2) window function:\n",
    "    https://www.wolframalpha.com/input/?i=y%3Dx**2,+y%3D-(x-2)**2+%2B2,+y%3D(x-4)**2,+from+y+%3D+0+to+2\n",
    "    \"\"\"\n",
    "    intersection = int(window_size/4)\n",
    "    wind_outer = (abs(2*(scipy.signal.triang(window_size))) ** power)/2\n",
    "    wind_outer[intersection:-intersection] = 0\n",
    "\n",
    "    wind_inner = 1 - (abs(2*(scipy.signal.triang(window_size) - 1)) ** power)/2\n",
    "    wind_inner[:intersection] = 0\n",
    "    wind_inner[-intersection:] = 0\n",
    "\n",
    "    wind = wind_inner + wind_outer\n",
    "    wind = wind / np.average(wind)\n",
    "    return wind\n",
    "\n",
    "\n",
    "cached_2d_windows = dict()\n",
    "def _window_2D(window_size, power=2):\n",
    "    \"\"\"\n",
    "    Make a 1D window function, then infer and return a 2D window function.\n",
    "    Done with an augmentation, and self multiplication with its transpose.\n",
    "    Could be generalized to more dimensions.\n",
    "    \"\"\"\n",
    "    # Memoization\n",
    "    global cached_2d_windows\n",
    "    key = \"{}_{}\".format(window_size, power)\n",
    "    if key in cached_2d_windows:\n",
    "        wind = cached_2d_windows[key]\n",
    "    else:\n",
    "        wind = _spline_window(window_size, power)\n",
    "        wind = np.expand_dims(np.expand_dims(wind, 1), 1)      #SREENI: Changed from 3, 3, to 1, 1 \n",
    "        wind = wind * wind.transpose(1, 0, 2)\n",
    "        if PLOT_PROGRESS:\n",
    "            # For demo purpose, let's look once at the window:\n",
    "            plt.imshow(wind[:, :, 0], cmap=\"viridis\")\n",
    "            plt.title(\"2D Windowing Function for a Smooth Blending of \"\n",
    "                      \"Overlapping Patches\")\n",
    "            plt.show()\n",
    "        cached_2d_windows[key] = wind\n",
    "    return wind\n",
    "\n",
    "\n",
    "def _pad_img(img, window_size, subdivisions):\n",
    "    \"\"\"\n",
    "    Add borders to img for a \"valid\" border pattern according to \"window_size\" and\n",
    "    \"subdivisions\".\n",
    "    Image is an np array of shape (x, y, nb_channels).\n",
    "    \"\"\"\n",
    "    aug = int(round(window_size * (1 - 1.0/subdivisions)))\n",
    "    more_borders = ((aug, aug), (aug, aug), (0, 0))\n",
    "    ret = np.pad(img, pad_width=more_borders, mode='reflect')\n",
    "    # gc.collect()\n",
    "\n",
    "    if PLOT_PROGRESS:\n",
    "        # For demo purpose, let's look once at the window:\n",
    "        plt.imshow(ret)\n",
    "        plt.title(\"Padded Image for Using Tiled Prediction Patches\\n\"\n",
    "                  \"(notice the reflection effect on the padded borders)\")\n",
    "        plt.show()\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _unpad_img(padded_img, window_size, subdivisions):\n",
    "    \"\"\"\n",
    "    Undo what's done in the `_pad_img` function.\n",
    "    Image is an np array of shape (x, y, nb_channels).\n",
    "    \"\"\"\n",
    "    aug = int(round(window_size * (1 - 1.0/subdivisions)))\n",
    "    ret = padded_img[\n",
    "        aug:-aug,\n",
    "        aug:-aug,\n",
    "        :\n",
    "    ]\n",
    "    # gc.collect()\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _rotate_mirror_do(im):\n",
    "    \"\"\"\n",
    "    Duplicate an np array (image) of shape (x, y, nb_channels) 8 times, in order\n",
    "    to have all the possible rotations and mirrors of that image that fits the\n",
    "    possible 90 degrees rotations.\n",
    "    It is the D_4 (D4) Dihedral group:\n",
    "    https://en.wikipedia.org/wiki/Dihedral_group\n",
    "    \"\"\"\n",
    "    mirrs = []\n",
    "    mirrs.append(np.array(im))\n",
    "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=1))\n",
    "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=2))\n",
    "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=3))\n",
    "    im = np.array(im)[:, ::-1]\n",
    "    mirrs.append(np.array(im))\n",
    "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=1))\n",
    "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=2))\n",
    "    mirrs.append(np.rot90(np.array(im), axes=(0, 1), k=3))\n",
    "    return mirrs\n",
    "\n",
    "\n",
    "def _rotate_mirror_undo(im_mirrs):\n",
    "    \"\"\"\n",
    "    merges a list of 8 np arrays (images) of shape (x, y, nb_channels) generated\n",
    "    from the `_rotate_mirror_do` function. Each images might have changed and\n",
    "    merging them implies to rotated them back in order and average things out.\n",
    "    It is the D_4 (D4) Dihedral group:\n",
    "    https://en.wikipedia.org/wiki/Dihedral_group\n",
    "    \"\"\"\n",
    "    origs = []\n",
    "    origs.append(np.array(im_mirrs[0]))\n",
    "    origs.append(np.rot90(np.array(im_mirrs[1]), axes=(0, 1), k=3))\n",
    "    origs.append(np.rot90(np.array(im_mirrs[2]), axes=(0, 1), k=2))\n",
    "    origs.append(np.rot90(np.array(im_mirrs[3]), axes=(0, 1), k=1))\n",
    "    origs.append(np.array(im_mirrs[4])[:, ::-1])\n",
    "    origs.append(np.rot90(np.array(im_mirrs[5]), axes=(0, 1), k=3)[:, ::-1])\n",
    "    origs.append(np.rot90(np.array(im_mirrs[6]), axes=(0, 1), k=2)[:, ::-1])\n",
    "    origs.append(np.rot90(np.array(im_mirrs[7]), axes=(0, 1), k=1)[:, ::-1])\n",
    "    return np.mean(origs, axis=0)\n",
    "\n",
    "\n",
    "def _windowed_subdivs(padded_img, window_size, subdivisions, nb_classes, pred_func):\n",
    "    \"\"\"\n",
    "    Create tiled overlapping patches.\n",
    "    Returns:\n",
    "        5D numpy array of shape = (\n",
    "            nb_patches_along_X,\n",
    "            nb_patches_along_Y,\n",
    "            patches_resolution_along_X,\n",
    "            patches_resolution_along_Y,\n",
    "            nb_output_channels\n",
    "        )\n",
    "    Note:\n",
    "        patches_resolution_along_X == patches_resolution_along_Y == window_size\n",
    "    \"\"\"\n",
    "    WINDOW_SPLINE_2D = _window_2D(window_size=window_size, power=2)\n",
    "\n",
    "    step = int(window_size/subdivisions)\n",
    "    padx_len = padded_img.shape[0]\n",
    "    pady_len = padded_img.shape[1]\n",
    "    subdivs = []\n",
    "\n",
    "    for i in range(0, padx_len-window_size+1, step):\n",
    "        subdivs.append([])\n",
    "        for j in range(0, pady_len-window_size+1, step):            #SREENI: Changed padx to pady (Bug in original code)\n",
    "            patch = padded_img[i:i+window_size, j:j+window_size, :]\n",
    "            subdivs[-1].append(patch)\n",
    "\n",
    "    # Here, `gc.collect()` clears RAM between operations.\n",
    "    # It should run faster if they are removed, if enough memory is available.\n",
    "    gc.collect()\n",
    "    subdivs = np.array(subdivs)\n",
    "    gc.collect()\n",
    "    a, b, c, d, e = subdivs.shape\n",
    "    subdivs = subdivs.reshape(a * b, c, d, e)\n",
    "    gc.collect()\n",
    "    print(subdivs.shape)\n",
    "    subdivs = pred_func(subdivs)\n",
    "    gc.collect()\n",
    "    subdivs = np.array([patch * WINDOW_SPLINE_2D for patch in subdivs])\n",
    "    gc.collect()\n",
    "\n",
    "    # Such 5D array:\n",
    "    subdivs = subdivs.reshape(a, b, c, d, nb_classes)\n",
    "    gc.collect()\n",
    "\n",
    "    return subdivs\n",
    "\n",
    "\n",
    "def _recreate_from_subdivs(subdivs, window_size, subdivisions, padded_out_shape):\n",
    "    \"\"\"\n",
    "    Merge tiled overlapping patches smoothly.\n",
    "    \"\"\"\n",
    "    step = int(window_size/subdivisions)\n",
    "    padx_len = padded_out_shape[0]\n",
    "    pady_len = padded_out_shape[1]\n",
    "\n",
    "    y = np.zeros(padded_out_shape)\n",
    "\n",
    "    a = 0\n",
    "    for i in range(0, padx_len-window_size+1, step):\n",
    "        b = 0\n",
    "        for j in range(0, pady_len-window_size+1, step):                #SREENI: Changed padx to pady (Bug in original code)\n",
    "            windowed_patch = subdivs[a, b]\n",
    "            y[i:i+window_size, j:j+window_size] = y[i:i+window_size, j:j+window_size] + windowed_patch\n",
    "            b += 1\n",
    "        a += 1\n",
    "    return y / (subdivisions ** 2)\n",
    "\n",
    "\n",
    "def predict_img_with_smooth_windowing(input_img, window_size, subdivisions, nb_classes, pred_func):\n",
    "    \"\"\"\n",
    "    Apply the `pred_func` function to square patches of the image, and overlap\n",
    "    the predictions to merge them smoothly.\n",
    "    See 6th, 7th and 8th idea here:\n",
    "    http://blog.kaggle.com/2017/05/09/dstl-satellite-imagery-competition-3rd-place-winners-interview-vladimir-sergey/\n",
    "    \"\"\"\n",
    "    pad = _pad_img(input_img, window_size, subdivisions)\n",
    "    pads = _rotate_mirror_do(pad)\n",
    "\n",
    "    # Note that the implementation could be more memory-efficient by merging\n",
    "    # the behavior of `_windowed_subdivs` and `_recreate_from_subdivs` into\n",
    "    # one loop doing in-place assignments to the new image matrix, rather than\n",
    "    # using a temporary 5D array.\n",
    "\n",
    "    # It would also be possible to allow different (and impure) window functions\n",
    "    # that might not tile well. Adding their weighting to another matrix could\n",
    "    # be done to later normalize the predictions correctly by dividing the whole\n",
    "    # reconstructed thing by this matrix of weightings - to normalize things\n",
    "    # back from an impure windowing function that would have badly weighted\n",
    "    # windows.\n",
    "\n",
    "    # For example, since the U-net of Kaggle's DSTL satellite imagery feature\n",
    "    # prediction challenge's 3rd place winners use a different window size for\n",
    "    # the input and output of the neural net's patches predictions, it would be\n",
    "    # possible to fake a full-size window which would in fact just have a narrow\n",
    "    # non-zero dommain. This may require to augment the `subdivisions` argument\n",
    "    # to 4 rather than 2.\n",
    "\n",
    "    res = []\n",
    "    for pad in tqdm(pads):\n",
    "        # For every rotation:\n",
    "        sd = _windowed_subdivs(pad, window_size, subdivisions, nb_classes, pred_func)\n",
    "        one_padded_result = _recreate_from_subdivs(\n",
    "            sd, window_size, subdivisions,\n",
    "            padded_out_shape=list(pad.shape[:-1])+[nb_classes])\n",
    "\n",
    "        res.append(one_padded_result)\n",
    "\n",
    "    # Merge after rotations:\n",
    "    padded_results = _rotate_mirror_undo(res)\n",
    "\n",
    "    prd = _unpad_img(padded_results, window_size, subdivisions)\n",
    "\n",
    "    prd = prd[:input_img.shape[0], :input_img.shape[1], :]\n",
    "\n",
    "    if PLOT_PROGRESS:\n",
    "        plt.imshow(prd)\n",
    "        plt.title(\"Smoothly Merged Patches that were Tiled Tighter\")\n",
    "        plt.show()\n",
    "    return prd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb256a4",
   "metadata": {},
   "source": [
    "# Using smooth predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615cf3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before pred:  (2646, 256, 256, 3)\n",
      "83/83 [==============================] - 257s 3s/step\n",
      "shape after pred:  (2646, 256, 256, 1)\n",
      "shape after nparray:  (2646, 256, 256, 1)\n",
      "shape after 5D:  (42, 63, 256, 256, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▍                                                                        | 1/8 [04:43<33:07, 283.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before pred:  (2646, 256, 256, 3)\n",
      "83/83 [==============================] - 250s 3s/step\n",
      "shape after pred:  (2646, 256, 256, 1)\n",
      "shape after nparray:  (2646, 256, 256, 1)\n",
      "shape after 5D:  (63, 42, 256, 256, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 2/8 [09:07<27:10, 271.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before pred:  (2646, 256, 256, 3)\n",
      "83/83 [==============================] - 241s 3s/step\n",
      "shape after pred:  (2646, 256, 256, 1)\n",
      "shape after nparray:  (2646, 256, 256, 1)\n",
      "shape after 5D:  (42, 63, 256, 256, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                   | 3/8 [13:23<22:03, 264.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before pred:  (2646, 256, 256, 3)\n",
      "20/83 [======>.......................] - ETA: 3:09"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████▏                                                   | 3/8 [14:34<24:17, 291.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m###################################################################################\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#Predict using smooth blending\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Use the algorithm. The `pred_func` is passed and will process all the image 8-fold by tiling small patches with overlap, called once with all those image as a batch outer dimension.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Note that model.predict(...) accepts a 4D tensor of shape (batch, x, y, nb_channels), such as a Keras model.\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m predictions_smooth \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_img_with_smooth_windowing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubdivisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Minimal amount of overlap for windowing. Must be an even number.\u001b[39;49;00m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnb_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg_batch_subdiv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_batch_subdiv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m final_prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions_smooth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#Save prediction and original mask for comparison\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Research\\UNET_Image_Segmentation\\smooth_tiled_predictions.py:239\u001b[0m, in \u001b[0;36mpredict_img_with_smooth_windowing\u001b[1;34m(input_img, window_size, subdivisions, nb_classes, pred_func)\u001b[0m\n\u001b[0;32m    236\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pad \u001b[38;5;129;01min\u001b[39;00m tqdm(pads):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# For every rotation:\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     sd \u001b[38;5;241m=\u001b[39m \u001b[43m_windowed_subdivs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubdivisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     one_padded_result \u001b[38;5;241m=\u001b[39m _recreate_from_subdivs(\n\u001b[0;32m    241\u001b[0m         sd, window_size, subdivisions,\n\u001b[0;32m    242\u001b[0m         padded_out_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(pad\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m+\u001b[39m[nb_classes])\n\u001b[0;32m    244\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(one_padded_result)\n",
      "File \u001b[1;32m~\\Documents\\Research\\UNET_Image_Segmentation\\smooth_tiled_predictions.py:172\u001b[0m, in \u001b[0;36m_windowed_subdivs\u001b[1;34m(padded_img, window_size, subdivisions, nb_classes, pred_func)\u001b[0m\n\u001b[0;32m    170\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape before pred: \u001b[39m\u001b[38;5;124m\"\u001b[39m, subdivs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 172\u001b[0m subdivs \u001b[38;5;241m=\u001b[39m \u001b[43mpred_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubdivs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape after pred: \u001b[39m\u001b[38;5;124m\"\u001b[39m, subdivs\u001b[38;5;241m.\u001b[39mshape)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(img_batch_subdiv)\u001b[0m\n\u001b[0;32m     33\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m###################################################################################\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#Predict using smooth blending\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Use the algorithm. The `pred_func` is passed and will process all the image 8-fold by tiling small patches with overlap, called once with all those image as a batch outer dimension.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Note that model.predict(...) accepts a 4D tensor of shape (batch, x, y, nb_channels), such as a Keras model.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m predictions_smooth \u001b[38;5;241m=\u001b[39m predict_img_with_smooth_windowing(\n\u001b[0;32m     42\u001b[0m     input_img,\n\u001b[0;32m     43\u001b[0m     window_size\u001b[38;5;241m=\u001b[39mpatch_size,\n\u001b[0;32m     44\u001b[0m     subdivisions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# Minimal amount of overlap for windowing. Must be an even number.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     nb_classes\u001b[38;5;241m=\u001b[39mn_classes,\n\u001b[0;32m     46\u001b[0m     pred_func\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m img_batch_subdiv: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_batch_subdiv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m final_prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions_smooth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#Save prediction and original mask for comparison\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2033\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   2032\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2033\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2034\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   2035\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "from PIL import Image\n",
    "import segmentation_models as sm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from smooth_tiled_predictions import predict_img_with_smooth_windowing\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "img = cv2.imread(\"../Datasets/Shoreline_Dataset/images/p1.JPG\")\n",
    "input_img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
    "input_img = preprocess_input(input_img)\n",
    "\n",
    "original_mask = cv2.imread(\"../Datasets/Shoreline_Dataset/masks/p1.png\")\n",
    "original_mask = original_mask[:,:,0]  #Use only single channel...\n",
    "#original_mask = to_categorical(original_mask, num_classes=n_classes)\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(\"landcover_25_epochs_RESNET_backbone_batch16.hdf5\", compile=False)\n",
    "                  \n",
    "# size of patches\n",
    "patch_size = 256\n",
    "\n",
    "# Number of classes \n",
    "n_classes = 1\n",
    "\n",
    "         \n",
    "###################################################################################\n",
    "#Predict using smooth blending\n",
    "\n",
    "# Use the algorithm. The `pred_func` is passed and will process all the image 8-fold by tiling small patches with overlap, called once with all those image as a batch outer dimension.\n",
    "# Note that model.predict(...) accepts a 4D tensor of shape (batch, x, y, nb_channels), such as a Keras model.\n",
    "predictions_smooth = predict_img_with_smooth_windowing(\n",
    "    input_img,\n",
    "    window_size=patch_size,\n",
    "    subdivisions=2,  # Minimal amount of overlap for windowing. Must be an even number.\n",
    "    nb_classes=n_classes,\n",
    "    pred_func=(\n",
    "        lambda img_batch_subdiv: model.predict((img_batch_subdiv))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "final_prediction = np.argmax(predictions_smooth, axis=2)\n",
    "\n",
    "#Save prediction and original mask for comparison\n",
    "plt.imsave('../Datasets/Shoreline_Dataset/predictions/prediction1.png', final_prediction)\n",
    "###################\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(221)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(img)\n",
    "plt.subplot(222)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(original_mask)\n",
    "plt.subplot(223)\n",
    "plt.title('Prediction with smooth blending')\n",
    "plt.imshow(final_prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4454d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
